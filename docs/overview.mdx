---
title: Overview
description: Cortex Overview.
slug: /
---

import OAICoverage from "@site/src/components/OAICoverage"

# Cortex

:::warning
ðŸš§ Cortex is under construction.
:::

![Cortex Cover Image](/img/social-card.jpg)

Cortex lets you run AI easily on your computer. 

We're a self-hosted alternative to the OpenAI Platform, built for on-prem and on-device use cases.

## Supported Accelerators
- Nvidia CUDA
- Apple Metal
- Qualcomm AI Engine

## Supported Inference Backends
- [llama.cpp](https://github.com/ggerganov/llama.cpp): cross-platform, supports most laptops, desktops and OSes
- [ONNX Runtime](https://github.com/microsoft/onnxruntime): supports Windows Copilot+ PCs & NPUs
- [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM): supports Nvidia GPUs 

If GPU hardware is available, Cortex is GPU accelerated by default.

:::info
**Real-world Use**: Cortex powers [Jan](https://jan.ai), our on-device ChatGPT-alternative.

Cortex has been battle-tested across 1 million+ downloads and handles a variety of hardware configurations.
:::

## Supported Models

Cortex supports the list of [Built-in Models](/models), such as:
- Mistral
- Llama
- Gemma
- Phi3

Cortex supports pulling `GGUF` and `ONNX` models from the [Hugging Face Hub](https://huggingface.co). Read how to [Pull models from Hugging Face](/docs/hub/hugging-face/)


## OpenAI API Equivalence

Cortex aims to be fully compatible with [OpenAI API](https://platform.openai.com/docs/api-reference) endpoints.

Our goal is to help developers to build applications with on-device AI and with a fully open-source stack. 

Cortex is currently compatible with the following OpenAI API endpoints:
- [/chat/completions](/api-reference#tag/inference/post/v1/chat/completions)
- [/embeddings](/api-reference#tag/embeddings/post/v1/embeddings)

<!--<OAICoverage /-->>
