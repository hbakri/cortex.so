---
title: Using Models
description: Model Operations
slug: "using-models"
---

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

:::warning
ðŸš§ Cortex is under construction.
:::

Cortex's Chat API is compatible with OpenAIâ€™s [Models API](https://platform.openai.com/docs/api-reference/models) endpoint. It is a drop-in replacement for model management. Additionally, Cortex exposes lower level operations for managing models like downloading models from a model hub and model loading.

:::info
Cortex also supports multiple model formats and architectures, including `GGUF`, `ONNX`, and `TensorRT-LLM`.
:::

## Usage

<Tabs>
  <TabItem  value="CLI" label="CLI" default>
    ```bash 
    cortex models
    ```
  </TabItem>
  {/* <TabItem value="Javascript" label="Javascript">
  </TabItem> */}
  <TabItem value="CURL" label="CURL">
  ## Load Models
    Cortex provides two ways to load your local model into the Cortex server.

    You can load the model using the CLI or the CURL following command:

      ### Request
      To load a model, use the following in your request:
      ```bash
        curl -X POST "http://localhost:1337/models/{janhq/TinyLlama-1.1B-Chat-v1.0-GGUF}/start" \
        -H "Content-Type: application/json" \
        -d '{
        "ctx_len": 512,
        "ngl": 4,
        "embedding": true,
        "n_parallel": 2,
        "cpu_threads": 8,
        "prompt_template": "Hello, this is a predefined text",
        "system_prompt": "System prompt text",
        "ai_prompt": "AI specific prompt text",
        "user_prompt": "User provided prompt text",
        "llama_model_path": "/path/to/llama/model",
        "mmproj": "Projection matrix details",
        "cont_batching": true,
        "vision_model": false,
        "text_model": true
        }'



      ```
      ### Endpoint Response
      Below are examples of the `start model` endpoint response from the Cortex server:
      ```bash
    {
      "message": "Model started successfully",
      "modelId": "janhq/TinyLlama-1.1B-Chat-v1.0-GGUF"
    }

      ```

    ## Unload Models
    Cortex provides two ways to unload your local model from the Cortex server.

    You can unload the model using the CLI or the CURL following command:

      ### Request
      To unload a model, use the following in your request:
      ```bash
        curl --request POST \
        --url http://localhost:1337/models/janhq/TinyLlama-1.1B-Chat-v1.0-GGUF/stop



      ```
      ### Endpoint Response
      Below are examples of the `stop model` endpoint response from the Cortex server:
      ```bash
    {
      "message": "Model stopped successfully",
      "modelId": "janhq/TinyLlama-1.1B-Chat-v1.0-GGUF"
    }

      ```
  </TabItem>
</Tabs>

## Capabilities

### Download Models
The Model Operations feature in Cortex allows you to download built-in models from our [Models hub](https://huggingface.co/cortexso). You can download any models in `GGUF`, `ONNX`, or `TensorRT-LLM` formats on [HuggingFace](https://huggingface.co/).
```bash
# Download a built-in model
cortex models pull mistral
# Download a specific variant
cortex models pull bartowski/Hermes-2-Theta-Llama-3-70B-GGUF
```

### Models Management
Cortex offers functionality to delete or update the configuration of a downloaded model. To do this, use the following command:
```bash
# Delete a model
cortex models remove
# Update a model
cortex models update
```