---
title: Using Models
description: Model Operations
slug: "using-models"
---

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

:::warning
ðŸš§ Cortex is under construction.
:::

Cortex's Models API is compatible with OpenAIâ€™s [Models API](https://platform.openai.com/docs/api-reference/models) endpoint. It is a fork of the OpenAI API used for model management. Additionally, Cortex exposes lower-level operations for managing models like downloading models from a model hub and model loading.

:::info
Cortex also supports multiple model formats and architectures, including `GGUF`, `ONNX`, and `TensorRT-LLM`.
:::

## Model Operations
Model Operations allows you to pull, run, and stop models.
### Run Model
:::info
The Run Model CLI and endpoint will automatically pull a model if it has not been downloaded yet.
:::
<Tabs>
  <TabItem  value="API" label="API" default>
    ```bash
        curl -X POST "http://localhost:1337/models/{janhq/TinyLlama-1.1B-Chat-v1.0-GGUF}/start" \
        -H "Content-Type: application/json" \
        -d '{
        "ctx_len": 512,
        "ngl": 4,
        "embedding": true,
        "n_parallel": 2,
        "cpu_threads": 8,
        "prompt_template": "Hello, this is a predefined text",
        "system_prompt": "System prompt text",
        "ai_prompt": "AI specific prompt text",
        "user_prompt": "User provided prompt text",
        "llama_model_path": "/path/to/llama/model",
        "mmproj": "Projection matrix details",
        "cont_batching": true,
        "vision_model": false,
        "text_model": true
        }'



      ```
  </TabItem>
  <TabItem  value="CLI" label="CLI" default>
    ```bash 
    cortex models run model_id
    ```
  </TabItem>
  <TabItem value="cortex.js" label="cortex.js">
  </TabItem>
  <TabItem value="cortex.py" label="cortex.py">
  </TabItem>
</Tabs>
### Stop Model
<Tabs>
  <TabItem  value="API-stop" label="API" default>
    ```bash
        curl --request POST \
        --url http://localhost:1337/models/janhq/TinyLlama-1.1B-Chat-v1.0-GGUF/stop



      ```
  </TabItem>
  <TabItem  value="CLI-stop" label="CLI" default>
    ```bash 
    cortex models stop model_id
    ```
  </TabItem>
  <TabItem value="cortex.js-stop" label="cortex.js">
  </TabItem>
  <TabItem value="cortex.py-stop" label="cortex.py">
  </TabItem>
</Tabs>
### Pull Model
<Tabs>
  <TabItem  value="API-pull" label="API" default>
    ```bash
        curl --request GET \
        --url 'http://localhost:1337/v1/models/download/TinyLlama-1.1B-Chat-v1.0-GGUF'



      ```
  </TabItem>
  <TabItem  value="CLI-pull" label="CLI" default>
    ```bash 
    # Download a built-in model
    cortex models pull mistral
    # Download a specific variant
    cortex models pull bartowski/Hermes-2-Theta-Llama-3-70B-GGUF
    ```
  </TabItem>
  <TabItem value="cortex.js-pull" label="cortex.js">
  </TabItem>
  <TabItem value="cortex.py-pull" label="cortex.py">
  </TabItem>
</Tabs>

## Models Management
Model Management allows you to manage your local models, which can be found in `~users/user_name/cortex/models`.
### List Models
<Tabs>
  <TabItem  value="API-pull" label="API" default>
    ```bash
        curl --request GET \
        --url http://localhost:1337/v1/models



      ```
  </TabItem>
  <TabItem  value="CLI-pull" label="CLI" default>
    ```bash 
    cortex models list
    ```
  </TabItem>
  <TabItem value="cortex.js-pull" label="cortex.js">
  </TabItem>
  <TabItem value="cortex.py-pull" label="cortex.py">
  </TabItem>
</Tabs>
### Get Model
<Tabs>
  <TabItem  value="API-pull" label="API" default>
    ```bash
        curl --request GET \
        --url http://localhost:1337/v1/models/TinyLlama-1.1B-Chat-v1.0-GGUF



      ```
  </TabItem>
  <TabItem  value="CLI-pull" label="CLI" default>
    ```bash 
    cortex models get model_id
    ```
  </TabItem>
  <TabItem value="cortex.js-pull" label="cortex.js">
  </TabItem>
  <TabItem value="cortex.py-pull" label="cortex.py">
  </TabItem>
</Tabs>
### Remove Model
<Tabs>
  <TabItem  value="API-pull" label="API" default>
    ```bash
        curl --request DELETE \
        --url http://localhost:1337/v1/models/TinyLlama-1.1B-Chat-v1.0-GGUF



      ```
  </TabItem>
  <TabItem  value="CLI-pull" label="CLI" default>
    ```bash 
    cortex models remove model_id
    ```
  </TabItem>
  <TabItem value="cortex.js-pull" label="cortex.js">
  </TabItem>
  <TabItem value="cortex.py-pull" label="cortex.py">
  </TabItem>
</Tabs>
### Update Model
<Tabs>
  <TabItem  value="API-pull" label="API" default>
    ```bash
        curl --request POST \
        --url http://localhost:1337/v1/models/TinyLlama-1.1B-Chat-v1.0-GGUF/config \
        --header 'Content-Type: application/json' \
        --data '{}'



      ```
  </TabItem>
  <TabItem  value="CLI-pull" label="CLI" default>
    ```bash 
    cortex models update
    ```
  </TabItem>
  <TabItem value="cortex.js-pull" label="cortex.js">
  </TabItem>
  <TabItem value="cortex.py-pull" label="cortex.py">
  </TabItem>
</Tabs>