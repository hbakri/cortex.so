---
title: TensorRT-LLM
description: NVIDIA TensorRT-LLM Architecture
slug: "cortex-tensorrt-llm"
---

:::warning
ðŸš§ Cortex is under construction.
:::

## Introduction

[Cortex.tensorrt-llm](https://github.com/janhq/cortex.tensorrt-llm) is a C++ inference library for NVIDIA GPUs. It submodules NVIDIAâ€™s [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) for GPU accelerated inference.

This guides walks you through how to use cortex.tensorrt-llm as a standalone library, in any custom C++ server.

In addition to TensorRT-LLM, Cortex.tensorrt-llm adds: 

- OpenAI object compatibility
- Tokenizers for popular model architectures
- Prebuilt model engines compatible with popular GPUs

:::info
If you use Cortex server, TensorRT-LLMâ€™s C++ runtime is bundled by default and you donâ€™t need this guide. Conversely, if you are interested to use TensorRT-LLM directly, here are some unofficial docs that might help. 
:::

## Usage
To include `cortex.tensorrt-llm` in your own server implementation, follow the steps [here](https://github.com/janhq/cortex.tensorrt-llm/tree/rel).


#### Get TensorRT-LLM Models

You can download precompiled models from the [Cortex Hub](https://huggingface.co/cortexso) on Hugging Face. These models include configurations, tokenizers, and dependencies tailored for optimal performance with this engine.

Read more about [model operations](./model-operations).