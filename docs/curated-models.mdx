---
title: Curated Models
description: Cortex Curated Models
---

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

Cortex works with most models out of the box. See [Hugging Face integration](./hub/hugging-face.mdx). 

Cortex also curates a list of popular models for your convenience. These models are compiled for popular inference engines, and include optimal runtime configurations (specified through [Model YAMLs](./model-yaml.mdx)).

You can find a full list of curated models [here](/models), or in the respective hubs: 
- [Hugging Face: Cortex Org](https://huggingface.co/cortexso)
- NGC: coming soon

:::info
Cortex aims to be compatible with existing model hubs. 
To request a new hub integration, ping us on [Discord](https://discord.gg/xsfBmpgTTj).
:::

## Model Variants

Curated models are made available across the following variants. 

#### By format
`GGUF`, `ONNX`, and `TensorRT-LLM`

#### By Size
`7b`, `13b`, and more.

#### By quantizations
`Q4`, `Q8`, and more.

Each model variant is stored in the model repository's **branches**, in Git fashion. 

## Run Model Variant

Variants can be run via Docker-like syntax:

```bash
cortex run MODEL-NAME:BRANCH-NAME
```

Or more verbosely, as follows:

<Tabs>
  <TabItem  value="GGUF" label="GGUF" default>

    ```bash 
    cortex pull llama3
    cortex pull llama3:gguf
    cortex pull llama3:8B-gguf
    ```
  </TabItem>
  <TabItem value="ONNX" label="ONNX">

    ```bash 
    cortex pull llama3:onnx
    ```
  </TabItem>
  <TabItem value="TensorRT-LLM" label="TensorRT-LLM">
    
    TensorRT-LLM engines are specific to the hardware you are running on.

    ```bash 
    cortex pull llama3:8B-tensorrtllm-linux-ada
    cortex pull llama3:tensorrtllm-windows-ada
    ```
  </TabItem>
</Tabs>

## Request Models

Join our Discord to request new preconfigured models.
https://discord.gg/7xt2yuepDe
