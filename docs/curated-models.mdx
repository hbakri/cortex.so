---
title: Curated Models
description: Cortex Curated Models
---

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

Cortex works with most models out of the box. Currently, we support [Hugging Face integration](./hub/hugging-face.mdx). 

We also curate popular models with optimal runtime configurations, specified through [Model YAMLs](./model-yaml.mdx).

## Model Hub
Cortex aims to be compatible with existing model hubs. Currently you can explore the full list of curated models [here](/models), or on our [Hugging Face: Cortex Org](https://huggingface.co/cortexso) hub.
To request a new hub integration, ping us on [Discord](https://discord.gg/xsfBmpgTTj).

:::warning
Cortex NGC Hub is coming soon! Stay tuned for updates.
:::

## Model Variants

Curated models are made available across the following variants: 

- **By format**: `GGUF`, `ONNX`, and `TensorRT-LLM`
- **By Size**: `7b`, `13b`, and more.
- **By quantizations**: `Q4`, `Q8`, and more.

:::info
Each model variant is stored in the model repository's **branches**, in Git fashion. 
:::
### Run Model Variant

Variants can be run via Docker-like syntax:

```bash
cortex run MODEL-NAME:BRANCH-NAME
```

Or more verbosely, as follows:

<Tabs>
  <TabItem  value="GGUF" label="GGUF" default>

    ```bash 
    cortex pull llama3
    cortex pull llama3:gguf
    cortex pull llama3:8B-gguf
    ```
  </TabItem>
  <TabItem value="ONNX" label="ONNX">

    ```bash 
    cortex pull llama3:onnx
    ```
  </TabItem>
  <TabItem value="TensorRT-LLM" label="TensorRT-LLM">
    
    TensorRT-LLM engines are specific to the hardware you are running on.

    ```bash 
    cortex pull llama3:8B-tensorrtllm-linux-ada
    cortex pull llama3:tensorrtllm-windows-ada
    ```
  </TabItem>
</Tabs>

## Request Models

Join our [Discord](https://discord.gg/7xt2yuepDe) to request new preconfigured models.
