---
title: Curated Models
description: Cortex Curated Models
---

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

Cortex works with most models out of the box. Currently, we support [Hugging Face integration](./hub/hugging-face.mdx). 

We also curate popular models with optimal runtime configurations, specified through [Model YAMLs](./model-yaml.mdx).

## Model Hub
Cortex aims to be compatible with existing model hubs. Currently you can explore the full list of curated models [here](/models), or on our [Hugging Face: Cortex Org](https://huggingface.co/cortexso) hub.
To request a new hub integration, ping us on [Discord](https://discord.gg/xsfBmpgTTj).

:::info
Cortex NGC Hub is coming soon! Stay tuned for updates.
:::

## Model Variants

Curated models are made available across the following variants: 

- **By format**: `gguf`, `onnx`, and `tensorrt-llm`
- **By Size**: `7b`, `13b`, and more.
- **By quantizations**: `q4`, `q8`, and more.

:::info
Each model variant is stored in the model repository's **branches**, in Git fashion. 
:::
### Run Model Variant

Variants can be run via Docker-like syntax:

```bash
cortex run MODEL-NAME:BRANCH-NAME
```

Or more verbosely, as follows:

<Tabs>
  <TabItem  value="GGUF" label="GGUF" default>

    ```bash 
    cortex pull mistral
    cortex pull mistral:gguf
    cortex pull mistral:7b-gguf
    ```
  </TabItem>
  <TabItem value="ONNX" label="ONNX">

    ```bash 
    cortex pull mistral:onnx
    ```
  </TabItem>
  <TabItem value="TensorRT-LLM" label="TensorRT-LLM">
    
    TensorRT-LLM engines are specific to the hardware you are running on.

    ```bash 
    cortex pull mistral:7b-tensorrt-llm
    ```
  </TabItem>
</Tabs>

## Request Models

Join our [Discord](https://discord.gg/7xt2yuepDe) to request new preconfigured models.
