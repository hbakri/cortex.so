---
title: Quickstart
description: Cortex Quickstart.
slug: quickstart
---

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";


:::warning
ðŸš§ Cortex is under construction.
:::

## Installation
```sh
# Install using Brew for Mac and Linux
brew install cortexso

# Install using NPM for Windows
npm i -g cortexso

```
## Run a Model
```sh
cortex run mistral
```
:::info
All model files are stored in the `~users/cortex/models` folder.
:::
## Using the Model
<Tabs>
  <TabItem  value="CLI" label="CLI" default>
    ```bash 
    cortex chat mistral
    ```
  </TabItem>
  <TabItem  value="API" label="API" default>
    ```bash
        curl http://localhost:1337/v1/chat/completions \
        -H "Content-Type: application/json" \
        -d '{
          "model": "",
          "messages": [
            {
              "role": "user",
              "content": "Hello"
            },
          ],
          "model": "mistral",
          "stream": true,
          "max_tokens": 1,
          "stop": [
              null
          ],
          "frequency_penalty": 1,
          "presence_penalty": 1,
          "temperature": 1,
          "top_p": 1
        }'

      ```
  </TabItem>
  <TabItem value="cortex.js" label="cortex.js">
  ```js
  const resp = await cortex.chat.completions.create({
    model: "mistral",
    messages: [
      { role: "system", content: "You are a chatbot." },
      { role: "user", content: "What is the capital of the United States?" },
    ],
  });
  ```
  </TabItem>
  <TabItem value="cortex.py" label="cortex.py">
  ```py
  completion = client.chat.completions.create(
    model=mistral,
    messages=[
        {
            "role": "user",
            "content": "Say this is a test",
        },
    ],
)
  ```
  </TabItem>
</Tabs>
## Stop a Model
 ```bash 
cortex stop
```
## Show the System State
 ```bash 
# Show a model and cortex system status
cortex ps

# Fetch telemetry logs to assess the cortex's performance, usage, and health.
cortex telemetry

# Show the Cortex logs
cortex logs
```
## Run Different Model Variants
:::info
All model files are stored in the `~users/cortex/models` folder.
:::
```bash
# Run HuggingFace model with HuggingFace Repo
cortex run TheBloke/Mistral-7B-Instruct-v0.2-GGUF

# Run Mistral in ONNX format
cortex run mistral:onnx

# Run Mistral in TensorRT-LLM format
cortex run mistral:tensorrt-llm
```
## Built-in Models

<Tabs>
  <TabItem  value="ONNX" label="ONNX" default>
| Model ID         | Variant (Branch) | Model size        | CLI command                        |
|------------------|------------------|-------------------|------------------------------------|
| gemma            | 7b-onnx          | 7B                | `cortex run gemma:7b-onnx`         |
| llama3           | onnx             | 8B                | `cortex run llama3:onnx`           |
| mistral          | 7b-onnx          | 7B                | `cortex run mistral:7b-onnx`       |
| openhermes-2.5   | 7b-onnx          | 7B                | `cortex run openhermes-2.5:7b-onnx`|
| phi3             | mini-onnx        | 3.82B - 4k ctx len| `cortex run phi3:mini-onnx`        |
| phi3             | medium-onnx      | 14B - 4k ctx len  | `cortex run phi3:medium-onnx`      |

  </TabItem>
  <TabItem  value="Llama.cpp" label="Llama.cpp" default>
| Model ID         | Variant (Branch) | Model size        | CLI command                        |
|------------------|------------------|-------------------|------------------------------------|
| codestral        | 22b-gguf         | 22B               | `cortex run codestral:22b-gguf`    |
| command-r        | 35b-gguf         | 35B               | `cortex run command-r:35b-gguf`    |
| gemma            | 7b-gguf          | 7B                | `cortex run gemma:7b-gguf`         |
| llama3           | gguf             | 8B                | `cortex run llama3:gguf`           |
| llama3.1         | gguf             | 8B                | `cortex run llama3.1:gguf`         |
| mistral          | 7b-gguf          | 7B                | `cortex run mistral:7b-gguf`       |
| mixtral          | 7x8b-gguf        | 46.7B             | `cortex run mixtral:7x8b-gguf`     |
| openhermes-2.5   | 7b-gguf          | 7B                | `cortex run openhermes-2.5:7b-gguf`|
| phi3             | medium-gguf      | 14B - 4k ctx len  | `cortex run phi3:medium-gguf`      |
| phi3             | mini-gguf        | 3.82B - 4k ctx len| `cortex run phi3:mini-gguf`        |
| qwen2            | 7b-gguf          | 7B                | `cortex run qwen2:7b-gguf`         |
| tinyllama        | 1b-gguf          | 1.1B              | `cortex run tinyllama:1b-gguf`     |
  </TabItem>
  <TabItem value="TensorRT-LLM" label="TensorRT-LLM">
| Model ID         | Variant (Branch)              | Model size        | CLI command                        |
|------------------|-------------------------------|-------------------|------------------------------------|
| llama3           | 8b-tensorrt-llm-windows-ampere       | 8B                | `cortex run llama3:8b-tensorrt-llm-windows-ampere`   |
| llama3           | 8b-tensorrt-llm-linux-ampere     | 8B                | `cortex run llama3:8b-tensorrt-llm-linux-ampere` |
| llama3           | 8b-tensorrt-llm-linux-ada   | 8B                | `cortex run llama3:8b-tensorrt-llm-linux-ada`|
| llama3           | 8b-tensorrt-llm-windows-ada       | 8B                | `cortex run llama3:8b-tensorrt-llm-windows-ada`   |
| mistral          | 7b-tensorrt-llm-linux-ampere     | 7B                | `cortex run mistral:7b-tensorrt-llm-linux-ampere`|
| mistral          | 7b-tensorrt-llm-windows-ampere       | 7B                | `cortex run mistral:7b-tensorrt-llm-windows-ampere`  |
| mistral          | 7b-tensorrt-llm-linux-ada   | 7B                | `cortex run mistral:7b-tensorrt-llm-linux-ada`|
| mistral          | 7b-tensorrt-llm-windows-ada       | 7B                | `cortex run mistral:7b-tensorrt-llm-windows-ada`  |
| openhermes-2.5   | 7b-tensorrt-llm-windows-ampere       | 7B                | `cortex run openhermes-2.5:7b-tensorrt-llm-windows-ampere`|
| openhermes-2.5   | 7b-tensorrt-llm-windows-ada     | 7B                | `cortex run openhermes-2.5:7b-tensorrt-llm-windows-ada`|
| openhermes-2.5   | 7b-tensorrt-llm-linux-ada   | 7B                | `cortex run openhermes-2.5:7b-tensorrt-llm-linux-ada`|

  </TabItem>
</Tabs>


:::info
Cortex is still in early development, so if you have any questions, please reach out to us:

- [GitHub](https://github.com/janhq/cortex)
- [Discord](https://discord.gg/YFKKeuVu)
  :::
