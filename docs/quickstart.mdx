---
title: Quickstart
description: Cortex Quickstart.
slug: quickstart
---

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";


:::warning
ðŸš§
- Cortex is still under construction.
- The current release will soon be deprecated in favor of a pure C++ implementation.
- READMEs and documentation may be out of sync as we focus on product development.
:::

## Installation
```sh
# Install using Brew for Mac
brew install cortex-engine

# Install using Winget for Windows
winget install cortex-engine

# Install using Sudo for Linux
sudo apt install cortex-engine

```
## Start Cortex Processes and API Server
This command starts the Cortex API server at `localhost:1337`.
```sh
cortex
```
## Run a Model
This command downloads the default `gguf` model format from the [Cortex Hub](https://huggingface.co/cortexso) and starts the model.
```sh
cortex run mistral
```
:::info
All model files are stored in the `~users/cortex/models` folder.
:::
## Using the Model
### CLI
```sh
# CLI
cortex chat mistral
```
### API
```curl
curl http://localhost:1337/v1/chat/completions \
-H "Content-Type: application/json" \
-d '{
  "model": "",
  "messages": [
    {
      "role": "user",
      "content": "Hello"
    },
  ],
  "model": "mistral",
  "stream": true,
  "max_tokens": 1,
  "stop": [
      null
  ],
  "frequency_penalty": 1,
  "presence_penalty": 1,
  "temperature": 1,
  "top_p": 1
}'
```
### Cortex.js
```js
const resp = await cortex.chat.completions.create({
    model: "mistral",
    messages: [
      { role: "system", content: "You are a chatbot." },
      { role: "user", content: "What is the capital of the United States?" },
    ],
  });
```
### Cortex-python
```py
completion = client.chat.completions.create(
    model=mistral,
    messages=[
        {
            "role": "user",
            "content": "Say this is a test",
        },
    ],
)
```
## Stop a Model
This command stops the running model and the Cortex API server.
 ```bash 
cortex stop
```
## Show the System State
This command displays the running model and the hardware system status.
 ```bash 
cortex ps
```
## Run Different Model Variants
:::info
All model files are stored in the `~users/cortex/models` folder.
:::
```bash
# Run HuggingFace model with HuggingFace Repo
cortex run TheBloke/Mistral-7B-Instruct-v0.2-GGUF

# Run Mistral in ONNX format
cortex run mistral:onnx

# Run Mistral in TensorRT-LLM format
cortex run mistral:tensorrt-llm
```


:::info
Cortex is still in early development, so if you have any questions, please reach out to us:

- [GitHub](https://github.com/janhq/cortex)
- [Discord](https://discord.gg/YFKKeuVu)
  :::
