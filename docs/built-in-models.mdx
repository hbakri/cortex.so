---
title: Built-in Models
description: Cortex Curated Models
---

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

Cortex maintains a collection of built-in models that cover the most popular open-source models.

## Cortex Model Repos
Built-in models are Cortex Model Repositories hosted on HuggingFace and pre-compiled for different engines, allowing one model to have multiple branches in various formats. We also plan to provide alternative hosting locations or servers to replicate the content, ensuring access in regions where HuggingFace is blocked or has slow download speeds.
:::info
For more information regarding our Cortex Model Repos, please see [here](/docs/hub/cortex-hub).
:::
## Model Variants
Built-in models are made available across the following variants: 

- **By format**: `gguf`, `onnx`, and `tensorrt-llm`
- **By Size**: `7b`, `13b`, and more.
- **By quantizations**: `q4`, `q8`, and more.

## Built-in Models List
You can see our full list of Built-in Models [here](/models). 

### Run Model 

Built-in models can be run via Docker-like syntax:

```bash
# Run a model
cortex run model-id
# Run a model variant
cortex run model-id:branch
```
For specific variant:

```bash
# Run Mistral Built-in Model
cortex pull mistral
# Run Mistral in GGUF format
cortex pull mistral:gguf
# Run Mistral in TensorRT-LLM format
cortex engines tensorrt-llm init
cortex pull mistral:7b-tensorrt-llm
# Run Mistral in ONNX format
cortex engines onnx init
cortex pull mistral:onnx
# Run Mistral with a different size
cortex pull mistral:7b-gguf

```