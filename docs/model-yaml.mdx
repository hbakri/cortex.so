---
title: Model YAML
description: The Model YAML
---

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

:::warning
ðŸš§ Cortex is under construction.
:::

Cortex automates model execution using parameters specified in the `model.yaml` file, which contains required runtime parameters and model settings. This flat, portable configuration file instructs Cortex on how to run the models.
:::info
Cortex requires a `model.yaml` file to run models. If a `model.yaml` file is not available, Cortex automatically generates one from the model metadata using default values.
:::

## Example YAMLs
Currently Cortex only supports the following model engine formats: **GGUF**, **ONNX**, and **TensorRT-LLM**.
:::info
`modelyaml` is different for each model format or model engine.
:::
<Tabs>
  <TabItem  value="GGUF" label="GGUF" default>
    **Example `model.yaml` for GGUF models**

    ```yaml
    name: mistral
    model: mistral:7b
    version: 1

    files:
      - llama_model_path: model.gguf

    # Results Preferences
    top_p: 0.95
    temperature: 0.7
    frequency_penalty: 0
    presence_penalty: 0
    max_tokens: 4096 # Infer from base config.json -> max_position_embeddings
    stream: true # true | false

    # Engine / Model Settings
    ngl: 33 # Infer from base config.json -> num_attention_heads
    ctx_len: 4096 # Infer from base config.json -> max_position_embeddings
    engine: cortex.llamacpp
    prompt_template: "{system_message} [INST] {prompt} [/INST]"

    ```
  </TabItem>
  <TabItem value="ONNX" label="ONNX">
    **Example `model.yaml` for `mistral 7b-onnx`**

    ```yaml
    name: mistral
    model: mistral:7b
    version: 1

    # files:
    files: []

    # Results Preferences
    top_p: 0.95
    temperature: 0.7
    frequency_penalty: 0
    presence_penalty: 0
    max_tokens: 4096 # Infer from base config.json -> max_position_embeddings
    stream: true # true | false

    # Engine / Model Settings
    ngl: 32 # Infer from base config.json -> num_attention_heads
    ctx_len: 4096 # Infer from base config.json -> max_position_embeddings
    engine: cortex.onnx
    prompt_template: "{system_message} [INST] {prompt} [/INST]"

    ```

  </TabItem>
  <TabItem value="TensorRT-LLM" label="TensorRT-LLM">
  **Example `model.yaml` for `mistral 7b-tensorrt-llm`**
  ```yaml
    name: mistral-v0.3
    model: mistral:7b-tensorrt-llm
    version: 1

    # Results Preferences
    temperature: 0.7
    max_tokens: 2048
    stream: true # true | false

    # Engine / Model Settings
    engine: cortex.tensorrt-llm
    os: windows
    gpu_arch: ada
    quantization_method: awq
    precision: int4
    tp: 1
    trtllm_version: 0.9.0
    ctx_len: 2048 # Infer from base config.json -> max_position_embeddings
    text_model: false
    ```
  </TabItem>
</Tabs>

## Model Parameters
`model.yaml` contains the all parameters needed to run models. Three main parameters are required in a `model.yaml`:
- Results Preferences
- Engine/Model Settings
- Template formats
<Tabs>
  <TabItem  value="GGUF" label="GGUF" default>
  | **Parameter**          | **Description**                                                                      |
  |------------------------|--------------------------------------------------------------------------------------|
  | `top_p`                | The cumulative probability threshold for token sampling.                             |
  | `temperature`          | Controls the randomness of predictions by scaling logits before applying softmax.    |
  | `frequency_penalty`    | Penalizes new tokens based on their existing frequency in the sequence so far.       |
  | `presence_penalty`     | Penalizes new tokens based on whether they appear in the sequence so far.            |
  | `max_tokens`           | Maximum number of tokens in the output; inferred from base config.json.              |
  | `stream`               | Enables or disables streaming mode for the output (true or false).                   |
  | `ngl`                  | Number of attention heads; inferred from base config.json.                           |
  | `ctx_len`              | Context length (maximum number of tokens); inferred from base config.json.           |
  | `engine`               | Specifies the engine to be used for model execution (e.g., cortex.llamacpp).         |
  | `prompt_template`      | Template for formatting the prompt, including system messages and instructions.      |
  | `end_token`            | Specifies the token that marks the end of the output.                                |

  </TabItem>
  <TabItem value="ONNX" label="ONNX">
  | **Parameter**          | **Description**                                                                      |
  |------------------------|--------------------------------------------------------------------------------------|
  | `top_p`                | The cumulative probability threshold for token sampling.                             |
  | `temperature`          | Controls the randomness of predictions by scaling logits before applying softmax.    |
  | `frequency_penalty`    | Penalizes new tokens based on their existing frequency in the sequence so far.       |
  | `presence_penalty`     | Penalizes new tokens based on whether they appear in the sequence so far.            |
  | `max_tokens`           | Maximum number of tokens in the output; inferred from base config.json.              |
  | `stream`               | Enables or disables streaming mode for the output (true or false).                   |
  | `ngl`                  | Number of attention heads; inferred from base config.json.                           |
  | `ctx_len`              | Context length (maximum number of tokens); inferred from base config.json.           |
  | `engine`               | Specifies the engine to be used for model execution (e.g., cortex.onnx).             |
  | `prompt_template`      | Template for formatting the prompt, including system messages and instructions.      |
  | `end_token`            | Specifies the token that marks the end of the output.                                |


  </TabItem>
  <TabItem value="TensorRT-LLM" label="TensorRT-LLM">

  :::info
  - `frequency_penalty`and `presence_penalty` is not available for the tensorrt-llm engine.
  - Template format parameters are hard-coded from the engine side.
  :::
  | **Parameter**          | **Description**                                                                      |
  |------------------------|--------------------------------------------------------------------------------------|
  | `temperature`          | Controls the randomness of predictions by scaling logits before applying softmax.     |
  | `max_tokens`           | Maximum number of tokens in the output; inferred from base config.json.               |
  | `stream`               | Enables or disables streaming mode for the output (true or false).                    |
  | `engine`               | Specifies the engine to be used for model execution (e.g., cortex.tensorrt-llm).      |
  | `os`                   | Operating system used (e.g., windows).                                               |
  | `gpu_arch`             | GPU architecture used (e.g., ada).                                                   |
  | `quantization_method`  | Method used for quantization (e.g., awq).                                            |
  | `precision`            | Precision level used (e.g., int4).                                                   |
  | `tp`                   | Number of tensor parallelism partitions.                                             |
  | `trtllm_version`       | Version of TensorRT-LLM being used.                                                  |
  | `ctx_len`              | Context length (maximum number of tokens); inferred from base config.json.            |
  | `text_model`           | Indicates if the text model is being used (true or false).                           |

  </TabItem>
</Tabs>
## Model Presets

Model presets are saved `model.yaml` files that serve as templates for pre-configured model settings. These presets are designed to ensure optimal performance with the specified engine.
These templates are not restricted to specific models. You can apply the presets to any model or any engine runtime.

:::info
Learn more about Models Presets:
- [Model Presets CLI](/docs/cli/presets)
:::
