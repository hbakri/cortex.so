---
title: Model YAML
description: The Model YAML
---

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

:::warning
ðŸš§ Cortex is under construction.
:::

Cortex requires a `model.yaml` file to execute models. The `yaml` files are flat, portable, and contain necessary runtime parameters and model settings.

:::info
If a `model.yaml` file is not available, Cortex automatically generates one from the model metadata using default values.
:::

## Example YAMLs
:::info
Currently Cortex only supports the following model format: **GGUF**, **ONNX**, and **TensorRT-LLM**.
:::
<Tabs>
  <TabItem  value="GGUF" label="GGUF" default>
    **Example `model.yaml` for GGUF models**

    ```yaml
    name: mistral
    model: mistral:7b
    version: 1

    files:
      - llama_model_path: model.gguf

    # Results Preferences
    top_p: 0.95
    temperature: 0.7
    frequency_penalty: 0
    presence_penalty: 0
    max_tokens: 4096 # Infer from base config.json -> max_position_embeddings
    stream: true # true | false

    # Engine / Model Settings
    ngl: 33 # Infer from base config.json -> num_attention_heads
    ctx_len: 4096 # Infer from base config.json -> max_position_embeddings
    engine: cortex.llamacpp
    prompt_template: "{system_message} [INST] {prompt} [/INST]"

    ```
  </TabItem>
  <TabItem value="ONNX" label="ONNX">
    **Example `model.yaml` for `mistral 7b-onnx`**

    ```yaml
    name: mistral
    model: mistral:7b
    version: 1

    # files:
    files: []

    # Results Preferences
    top_p: 0.95
    temperature: 0.7
    frequency_penalty: 0
    presence_penalty: 0
    max_tokens: 4096 # Infer from base config.json -> max_position_embeddings
    stream: true # true | false

    # Engine / Model Settings
    ngl: 32 # Infer from base config.json -> num_attention_heads
    ctx_len: 4096 # Infer from base config.json -> max_position_embeddings
    engine: cortex.onnx
    prompt_template: "{system_message} [INST] {prompt} [/INST]"

    ```

  </TabItem>
  <TabItem value="TensorRT-LLM" label="TensorRT-LLM">
  **Example `model.yaml` for `mistral 7b-tensorrt-llm`**
  ```yaml
    name: mistral-v0.3
    model: mistral:7b-tensorrt-llm
    version: 1

    # Results Preferences
    temperature: 0.7
    max_tokens: 2048
    stream: true # true | false

    # Engine / Model Settings
    engine: cortex.tensorrt-llm
    os: windows
    gpu_arch: ada
    quantization_method: awq
    precision: int4
    tp: 1
    trtllm_version: 0.9.0
    ctx_len: 2048 # Infer from base config.json -> max_position_embeddings
    text_model: false
    ```
  </TabItem>
</Tabs>

## Model Presets

Cortex offers a `model.yaml` file that acts as a preset, which are template for model settings, for popular model architectures like **ChatML**. These presets are pre-configured to ensure the model runs optimally with the specified engine.

These templates are not restricted to specific models. You can apply the settings in the `model.yaml` to any model.

:::info
Learn more about Models capabilities:
- [Model Object](/api-reference#tag/models/get/models/{id})
- [Model API](/api-reference#tag/models/post/models)
- [Model CLI](/docs/cli/models/)
:::
