---
title: model.yaml
description: The model.yaml
---

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

:::warning
ðŸš§ Cortex is under construction.
:::

Cortex uses a `model.yaml` file to specify the configuration for running a model. This file provides Cortex with information about the engine to be used and the parameters for running the model.


## `model.yaml` High Level Structure
Here is an example of `model.yaml` format:
```yaml
# Cortex Meta
name: 
model: 
version: 

# Engine / Model Settings
engine: 
ngl:  
ctx_len: 
prompt_template: 

# Results Preferences
stop:
max_tokens: 
stream: 

```

The `model.yaml` is composed of three high-level sections:

### Cortex Meta
```yaml
# Cortex Meta
name: openhermes-2.5
model: openhermes-2.5:7B
version: 1
```
Cortex Meta consists of essential metadata that identifies the model within Cortex. The required parameters include:
| **Parameter** | **Description** |
|---------------|-----------------|
| `name`        | The identifier name of the model, used as the `model_id`. |
| `model`       | Details specifying the variant of the model, including size or quantization. |
| `version`     | The version number of the model. |

### Engine / Model Settings
```yaml
# Engine / Model Settings
engine: cortex.llamacpp
ngl: 33 
ctx_len: 4096 
prompt_template: "<|im_start|>system\n{system_message}<|im_end|>\n<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"

```
Engine/Model Settings include the options that control how Cortex runs the model. The required parameters include:
| **Parameter** | **Description** |
|---------------|-----------------|
| `engine`               | Specifies the engine to be used for model execution.         |
| `ngl`                  | Number of attention heads.                            |
| `ctx_len`              | Context length (maximum number of tokens).            |
| `prompt_template`      | Template for formatting the prompt, including system messages and instructions.      |

### Result Preferences
```yaml
# Results Preferences
stop:
  - </s>
max_tokens: 4096 
stream: true 
```
Result Preferences define how the results will be produced. The required parameters include:
| **Parameter** | **Description** |
|---------------|-----------------|
| `max_tokens`           | Maximum number of tokens in the output.              |
| `stream`               | Enables or disables streaming mode for the output (true or false).                   |
| `stop`               | Specifies the stopping condition for the model, which can be a word, a letter, or a specific text. |

:::info
The `model.yaml` parameters vary for each model type. To view the specific format for `model.yaml` for different model types, refer to:

- [`GGUF`](/docs/formats/gguf)
- [`TensorRT-LLM`](/docs/formats/tensorrt-llm)
- [`ONNX`](/docs/formats/onnx)

:::
