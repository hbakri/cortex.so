---
title: Cortex Server
description: Cortex Server Overview.
slug: "server"
---

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

:::warning
ðŸš§ Cortex is under construction.
:::

Cortex provides an API server that is [OpenAI Equivalence](https://platform.openai.com/docs/api-reference/introduction). We plan to provide a complete open-source implementation of the OpenAI API, including all endpoints.

:::info
We are soon implementing the Assistants and Fine-Tuning API endpoints.
:::

## Usage
1. Start a cortex server:
```bash
# By default the server will be started on port `1337`
cortex
# Start a server with different port number
cortex -a address -p port_number
# Start with a specified data folder
cortex --dataFolder
```
2. Run a model:
```sh
curl --request POST \
  --url http://localhost:1337/v1/models/mistral/start \
  --header 'Content-Type: application/json' \
  --data '{
  "prompt_template": "system\n{system_message}\nuser\n{prompt}\nassistant",
  "stop": [],
  "ngl": 4096,
  "ctx_len": 4096,
  "cpu_threads": 10,
  "n_batch": 2048,
  "caching_enabled": true,
  "grp_attn_n": 1,
  "grp_attn_w": 512,
  "mlock": false,
  "flash_attn": true,
  "cache_type": "f16",
  "use_mmap": true,
  "engine": "cortex.llamacpp"
}'
```
3. Invoke the chat completions endpoint:
```json
curl http://localhost:1337/v1/chat/completions \
-H "Content-Type: application/json" \
-d '{
  "model": "",
  "messages": [
    {
      "role": "user",
      "content": "Hello"
    },
  ],
  "model": "",
  "stream": true,
  "max_tokens": 1,
  "stop": [
      null
  ],
  "frequency_penalty": 1,
  "presence_penalty": 1,
  "temperature": 1,
  "top_p": 1
}'
```
4. Stop a model:
```json
curl --request POST \
  --url http://localhost:1337/v1/models/mistral/stop
```

:::info
For a full list of our API endpoints, please see our [API Reference](https://cortex.so/api-reference).
:::